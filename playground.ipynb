{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2074778b",
   "metadata": {},
   "source": [
    "# **Lottery Ticket Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68303f3e",
   "metadata": {},
   "source": [
    "Within this notebook, I would like to demonstrate how I can catch the winning lottery in order to find subnetwork inside a large network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9249c4d",
   "metadata": {},
   "source": [
    "## **1. Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a35c6",
   "metadata": {},
   "source": [
    "Load packages needed and a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1ecb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311aee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [01:21<00:00, 324kB/s] \n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 169kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:12<00:00, 350kB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset (MNIST dataset)\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "testing_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Save training and testing data\n",
    "torch.save(training_data, \"./data/mnist_training_data.pt\")\n",
    "torch.save(testing_data, \"./data/mnist_testing_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e70a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and testing dataset\n",
    "training_data = torch.load(\"./data/mnist_training_data.pt\", weights_only=False)\n",
    "testing_data = torch.load(\"./data/mnist_testing_data.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "773ee412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap training and testing data into data loader\n",
    "training_dataloader = DataLoader(training_data, batch_size=128, shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae99f70",
   "metadata": {},
   "source": [
    "## **Define MLP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d984d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(784, 256)\n",
    "        self.linear_2 = nn.Linear(256, 128)\n",
    "        self.linear_3 = nn.Linear(128, 32)\n",
    "        self.linear_4 = nn.Linear(32, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear_1(x)\n",
    "        y = self.relu(y)\n",
    "        y = self.linear_2(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.linear_3(y)\n",
    "        y = self.relu(y)\n",
    "        output = self.linear_4(y)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8c3a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear_1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (linear_2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (linear_3): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (linear_4): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8df82b",
   "metadata": {},
   "source": [
    "# **Find Winning Ticket**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65d970",
   "metadata": {},
   "source": [
    "In order to find winning ticket which is the subnetwork that maximally contribute to the result, we need to copy the initial parameters of the model (weights and biases) which then be used to train pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59a9a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Copy initial parameters\n",
    "model_parameter_copy = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7d8f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(lr=0.0001, params=model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fa65646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          training_data_loader,\n",
    "          testing_data_loader,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          epoch):\n",
    "    for ep in range(epoch):\n",
    "        # Training\n",
    "        model.train()\n",
    "        # Batch processing\n",
    "        for x_batch, y_batch in training_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_batch = x_batch.view(x_batch.size(0), -1)\n",
    "            prediction = model.forward(x_batch)\n",
    "            loss = loss_fn(prediction, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_test_batch, y_test_batch in testing_data_loader:\n",
    "                x_test_batch = x_test_batch.view(x_test_batch.size(0), -1)\n",
    "                eval_output = model.forward(x_test_batch)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            loss_test = loss_fn(eval_output, y_test_batch)\n",
    "            eval_output_labeled = torch.argmax(eval_output, dim=1)\n",
    "            accuracy = (eval_output_labeled == y_test_batch).sum().item() / len(eval_output_labeled)\n",
    "\n",
    "        print(f\"Epoch {ep+1} ==> Loss = {loss_test} | Accuracy = {round(accuracy, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bea945de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ==> Loss = 0.4657773971557617 | Accuracy = 0.75\n",
      "Epoch 2 ==> Loss = 0.27131006121635437 | Accuracy = 0.94\n",
      "Epoch 3 ==> Loss = 0.5274918675422668 | Accuracy = 0.88\n",
      "Epoch 4 ==> Loss = 0.3444649875164032 | Accuracy = 0.94\n",
      "Epoch 5 ==> Loss = 0.3275434672832489 | Accuracy = 0.88\n",
      "Epoch 6 ==> Loss = 0.3738173544406891 | Accuracy = 0.81\n",
      "Epoch 7 ==> Loss = 0.5602895021438599 | Accuracy = 0.81\n",
      "Epoch 8 ==> Loss = 0.3862707018852234 | Accuracy = 0.75\n",
      "Epoch 9 ==> Loss = 0.31783944368362427 | Accuracy = 0.94\n",
      "Epoch 10 ==> Loss = 0.3195052742958069 | Accuracy = 0.81\n",
      "Epoch 11 ==> Loss = 0.7905294299125671 | Accuracy = 0.75\n",
      "Epoch 12 ==> Loss = 0.3970736265182495 | Accuracy = 0.88\n",
      "Epoch 13 ==> Loss = 0.42180317640304565 | Accuracy = 0.75\n",
      "Epoch 14 ==> Loss = 0.6111569404602051 | Accuracy = 0.62\n",
      "Epoch 15 ==> Loss = 0.3974618911743164 | Accuracy = 0.88\n",
      "Epoch 16 ==> Loss = 0.3167000114917755 | Accuracy = 0.94\n",
      "Epoch 17 ==> Loss = 0.21949374675750732 | Accuracy = 0.94\n",
      "Epoch 18 ==> Loss = 0.5956071615219116 | Accuracy = 0.75\n",
      "Epoch 19 ==> Loss = 0.2315887212753296 | Accuracy = 0.88\n",
      "Epoch 20 ==> Loss = 0.47589603066444397 | Accuracy = 0.81\n",
      "Epoch 21 ==> Loss = 0.08064345270395279 | Accuracy = 1.0\n",
      "Epoch 22 ==> Loss = 0.46388116478919983 | Accuracy = 0.81\n",
      "Epoch 23 ==> Loss = 0.4066625237464905 | Accuracy = 0.81\n",
      "Epoch 24 ==> Loss = 0.42638760805130005 | Accuracy = 0.75\n",
      "Epoch 25 ==> Loss = 0.26731687784194946 | Accuracy = 0.88\n",
      "Epoch 26 ==> Loss = 0.7467104196548462 | Accuracy = 0.69\n",
      "Epoch 27 ==> Loss = 0.32174253463745117 | Accuracy = 0.88\n",
      "Epoch 28 ==> Loss = 0.1752757728099823 | Accuracy = 1.0\n",
      "Epoch 29 ==> Loss = 0.6607188582420349 | Accuracy = 0.81\n",
      "Epoch 30 ==> Loss = 0.3178744912147522 | Accuracy = 0.88\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train(\n",
    "    model,\n",
    "    training_dataloader,\n",
    "    testing_dataloader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5de55",
   "metadata": {},
   "source": [
    "# **Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09774844",
   "metadata": {},
   "source": [
    "After training the model and achieving the highest accuracy and the lowest loss value, then we perform pruning to create a sparse matrix (turn off the effect of few neurons within the big network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "470e54c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_1.weight',\n",
       "              tensor([[ 0.0950,  0.0353,  0.0237,  ..., -0.0350,  0.0286,  0.0392],\n",
       "                      [-0.0019, -0.0829,  0.0020,  ...,  0.0378,  0.0912,  0.0309],\n",
       "                      [-0.0643, -0.0754, -0.0452,  ..., -0.0726, -0.1302, -0.0625],\n",
       "                      ...,\n",
       "                      [-0.0724, -0.0226, -0.0903,  ...,  0.0350,  0.0261, -0.0487],\n",
       "                      [ 0.0541, -0.0775, -0.0612,  ..., -0.0666,  0.0562, -0.0386],\n",
       "                      [ 0.0415,  0.0179, -0.0985,  ..., -0.0396, -0.0039, -0.0418]])),\n",
       "             ('linear_1.bias',\n",
       "              tensor([ 2.0831e-01, -2.1906e-02,  4.1102e-02, -6.6021e-02, -3.6502e-02,\n",
       "                       2.8309e-03,  4.5656e-02, -3.7591e-02, -3.4435e-02, -4.4073e-02,\n",
       "                       2.0710e-02, -1.2697e-01,  1.0828e-02,  1.1019e-01,  4.3496e-03,\n",
       "                       1.7435e-01,  1.3645e-01,  4.6424e-02, -3.3416e-02,  3.0224e-01,\n",
       "                      -2.4514e-01,  1.1537e-01,  3.1745e-02,  1.8782e-01,  7.1748e-02,\n",
       "                       6.9810e-02,  3.8171e-02,  9.6300e-02, -2.6342e-02,  5.9144e-02,\n",
       "                      -1.3105e-01,  1.6288e-01, -4.4566e-02, -1.0080e-01,  4.6183e-02,\n",
       "                       4.4561e-02,  6.9640e-02, -1.9696e-02, -5.0873e-02,  9.1190e-02,\n",
       "                       5.0227e-03, -2.1194e-02, -1.8817e-02,  4.2949e-02,  8.3144e-03,\n",
       "                       3.7185e-02,  1.0158e-01, -4.3520e-02,  5.7717e-02,  9.2958e-02,\n",
       "                       1.9421e-02, -4.1240e-02, -3.5855e-02,  4.2309e-02,  8.9072e-02,\n",
       "                       2.5904e-02, -4.8630e-02,  6.9802e-02,  9.5462e-02,  4.7465e-02,\n",
       "                       1.7486e-01, -1.5489e-03,  9.8336e-03, -5.3384e-02,  5.0363e-02,\n",
       "                       6.1769e-02,  9.0447e-02, -5.1240e-02,  2.2755e-02, -6.3353e-02,\n",
       "                       4.0104e-02,  2.4361e-02,  3.2965e-04, -1.7371e-01, -9.3739e-03,\n",
       "                       1.1761e-01,  2.9562e-02, -7.1733e-02,  6.1905e-02,  8.6663e-02,\n",
       "                       1.1283e-01,  9.8451e-02,  7.5809e-02,  6.0932e-02, -3.8973e-02,\n",
       "                      -7.6128e-02,  1.0953e-01, -3.0884e-02,  1.1268e-01,  7.2977e-02,\n",
       "                       3.4586e-02, -5.2746e-02,  9.4172e-02,  1.2562e-01, -8.0788e-02,\n",
       "                      -2.3597e-02, -5.3058e-02,  6.4265e-02,  9.3932e-02, -3.6464e-02,\n",
       "                       8.0575e-03,  1.8028e-01,  1.0813e-02, -2.1912e-02,  1.2326e-01,\n",
       "                       1.7652e-02,  1.6211e-01,  4.2556e-02,  2.7262e-02, -1.8985e-02,\n",
       "                      -1.0937e-02,  1.0194e-02,  7.2140e-02,  1.1142e-01,  1.2357e-01,\n",
       "                       4.6826e-02,  2.0348e-03, -8.0751e-02, -6.3275e-03,  5.9856e-02,\n",
       "                       6.6909e-02,  6.7775e-02,  1.2074e-01, -3.4098e-02,  8.6744e-02,\n",
       "                       1.0831e-02,  3.1226e-05, -1.4002e-02,  7.3829e-02, -1.2834e-01,\n",
       "                       3.7347e-02,  1.4915e-01, -1.7156e-02,  9.1657e-02, -1.8220e-02,\n",
       "                       2.3701e-02,  7.8859e-02,  1.4680e-02, -5.7241e-02,  3.0019e-02,\n",
       "                       1.8048e-02,  1.3989e-02,  5.5645e-02,  3.2028e-02, -2.0663e-02,\n",
       "                       6.8289e-02,  4.1836e-02, -2.0455e-02,  4.4359e-02,  9.9568e-02,\n",
       "                       1.1892e-01,  1.2051e-01, -9.9061e-03,  1.3584e-01, -2.7104e-03,\n",
       "                      -8.3805e-03, -8.0822e-02,  6.5473e-02,  2.4324e-02, -1.0824e-01,\n",
       "                       1.2616e-01, -5.6514e-02,  3.4270e-03,  8.1665e-02,  1.1606e-02,\n",
       "                      -1.6202e-02, -8.5765e-04, -3.0383e-04, -3.6994e-02,  3.7733e-02,\n",
       "                       4.1110e-03, -6.7256e-02,  2.7990e-02,  8.4855e-02,  2.0173e-01,\n",
       "                       1.5445e-02,  2.9792e-02,  5.2371e-02, -6.6559e-02, -2.7690e-02,\n",
       "                      -5.0713e-02,  1.1409e-01,  3.6224e-02,  8.2893e-02,  4.1552e-02,\n",
       "                      -3.5890e-02,  3.1167e-02, -2.5345e-02,  4.7043e-02, -4.2975e-02,\n",
       "                       1.0798e-01, -6.2903e-03,  1.3070e-01, -3.7411e-02, -4.1536e-02,\n",
       "                      -8.2549e-03,  2.1488e-01,  1.6424e-01,  2.9213e-02,  1.9039e-01,\n",
       "                       1.1160e-01,  2.0328e-01, -9.9565e-03,  7.8236e-02, -9.3225e-02,\n",
       "                      -1.8498e-02,  2.1804e-01,  3.0394e-02,  4.9573e-03, -8.3463e-03,\n",
       "                       1.4855e-01,  8.5468e-02, -2.7448e-03,  1.4853e-01,  1.3592e-01,\n",
       "                       6.4631e-03,  5.4941e-02,  8.5173e-02,  8.3824e-02, -9.3079e-02,\n",
       "                       3.7276e-02,  1.4877e-01, -7.5197e-02, -1.7488e-02,  8.2952e-02,\n",
       "                       1.6451e-01,  8.6441e-02,  4.7446e-02, -2.7876e-03,  1.3815e-01,\n",
       "                      -3.7445e-02,  9.3446e-03,  1.6848e-02, -3.3362e-03,  3.0047e-02,\n",
       "                       3.9414e-02, -8.5678e-02,  4.4577e-02,  1.1983e-02,  7.5498e-02,\n",
       "                       1.0694e-01,  2.9359e-02,  7.6730e-02,  8.3276e-02, -1.4017e-02,\n",
       "                      -1.9898e-04, -1.7315e-01,  6.0177e-02,  1.2704e-01, -3.1585e-02,\n",
       "                      -9.3925e-02, -1.3354e-01, -3.2331e-03, -1.6123e-02, -1.3970e-02,\n",
       "                      -5.6853e-02])),\n",
       "             ('linear_2.weight',\n",
       "              tensor([[-0.0349,  0.0017,  0.0764,  ..., -0.0062, -0.0112, -0.0544],\n",
       "                      [-0.1089, -0.0621,  0.0689,  ...,  0.0355, -0.0285, -0.0421],\n",
       "                      [ 0.0172,  0.0062,  0.0382,  ...,  0.0166,  0.0679, -0.0628],\n",
       "                      ...,\n",
       "                      [ 0.0525,  0.0093,  0.0049,  ...,  0.0238, -0.0303,  0.0851],\n",
       "                      [-0.0476,  0.0748,  0.0532,  ...,  0.0793,  0.0533, -0.0546],\n",
       "                      [-0.0472,  0.0047,  0.0411,  ...,  0.0155,  0.0444,  0.0493]])),\n",
       "             ('linear_2.bias',\n",
       "              tensor([ 0.0054, -0.0275, -0.0301,  0.0583, -0.0395, -0.0043, -0.0765, -0.0275,\n",
       "                      -0.0186, -0.0434, -0.0420,  0.1306, -0.1059,  0.0491, -0.1058,  0.0954,\n",
       "                      -0.1457, -0.0643,  0.0758,  0.1494,  0.1462,  0.0149,  0.0016,  0.0082,\n",
       "                       0.0335, -0.1176, -0.0492,  0.1768,  0.0974, -0.0311,  0.0433,  0.0360,\n",
       "                       0.0132,  0.1071, -0.0579, -0.0247,  0.0937, -0.2493, -0.0613,  0.0814,\n",
       "                       0.0415, -0.0116, -0.0293,  0.0366,  0.0773,  0.1551, -0.0305,  0.0259,\n",
       "                      -0.0196, -0.1198,  0.1782,  0.0997,  0.0879,  0.0687,  0.1099,  0.1255,\n",
       "                       0.0329, -0.1440, -0.0410,  0.0144, -0.0585, -0.2047, -0.0745,  0.0734,\n",
       "                      -0.0300, -0.0731,  0.0837,  0.0961,  0.0534,  0.0466, -0.1527, -0.0411,\n",
       "                      -0.0540,  0.0527,  0.0086, -0.0553, -0.0482, -0.0199,  0.2354,  0.1751,\n",
       "                       0.1276,  0.0688, -0.0535, -0.0204, -0.0135, -0.0568,  0.0887,  0.0503,\n",
       "                       0.0410,  0.0678,  0.0670,  0.0607,  0.1432, -0.0168, -0.0179, -0.0570,\n",
       "                      -0.0427,  0.1472,  0.0021,  0.0652,  0.0780, -0.1356,  0.2597,  0.0282,\n",
       "                      -0.0606,  0.0380,  0.0741,  0.2558,  0.1055,  0.0889, -0.0168, -0.1428,\n",
       "                       0.1903, -0.0442,  0.0441,  0.0403, -0.0481,  0.0432, -0.0382,  0.0104,\n",
       "                       0.0223,  0.1242,  0.1320,  0.0335,  0.0091,  0.0613, -0.0087, -0.1133])),\n",
       "             ('linear_3.weight',\n",
       "              tensor([[-0.0532,  0.0516, -0.0820,  ...,  0.0117,  0.0269, -0.0648],\n",
       "                      [-0.0354,  0.0586, -0.0468,  ..., -0.0460,  0.0060, -0.0588],\n",
       "                      [ 0.0717,  0.1390, -0.0165,  ...,  0.0003, -0.0139,  0.0534],\n",
       "                      ...,\n",
       "                      [ 0.0202, -0.1605, -0.0045,  ..., -0.0212, -0.1324,  0.0069],\n",
       "                      [-0.0058,  0.0735,  0.0553,  ..., -0.0421,  0.0755,  0.0877],\n",
       "                      [-0.0049, -0.0994,  0.0393,  ..., -0.0297, -0.0131, -0.0759]])),\n",
       "             ('linear_3.bias',\n",
       "              tensor([ 0.0087, -0.0596, -0.1583,  0.0639,  0.0640, -0.0251,  0.0523, -0.0896,\n",
       "                       0.0754, -0.1632,  0.2041,  0.0034,  0.0155, -0.0851,  0.0390,  0.0183,\n",
       "                      -0.0185,  0.0624,  0.0466, -0.0563,  0.0268,  0.0438,  0.0615, -0.1003,\n",
       "                      -0.1095,  0.1297,  0.2121,  0.1040,  0.0183,  0.0796, -0.0647,  0.1886])),\n",
       "             ('linear_4.weight',\n",
       "              tensor([[ 0.0028, -0.0791, -0.1900,  0.0186,  0.1804, -0.0636, -0.1733, -0.0797,\n",
       "                        0.0656,  0.1761,  0.0498,  0.1065, -0.2504,  0.0647, -0.2720, -0.0065,\n",
       "                        0.1117,  0.1532, -0.2115,  0.0173, -0.0985, -0.0572,  0.1266,  0.1868,\n",
       "                        0.0918, -0.1729, -0.2314,  0.0482,  0.2166,  0.0233, -0.0925,  0.0133],\n",
       "                      [-0.0750, -0.0441,  0.0352,  0.1733, -0.0801, -0.1216, -0.0669, -0.1936,\n",
       "                        0.1952,  0.0200, -0.3134, -0.0168,  0.0094, -0.0746,  0.1219, -0.1218,\n",
       "                       -0.0945, -0.2718, -0.0447,  0.2203,  0.0202, -0.0931, -0.0336, -0.1356,\n",
       "                        0.1747,  0.0357,  0.1309, -0.2880,  0.0037, -0.2175, -0.0226, -0.0059],\n",
       "                      [ 0.1536,  0.0874,  0.0311, -0.2124,  0.0868, -0.1359, -0.0872, -0.1166,\n",
       "                        0.0777, -0.1130, -0.1094,  0.0481,  0.0377,  0.1471,  0.0789,  0.1202,\n",
       "                       -0.0418, -0.0640, -0.0086, -0.0951, -0.1126,  0.1818,  0.1655,  0.1322,\n",
       "                       -0.2219, -0.0473, -0.1954,  0.0497,  0.2327, -0.1496,  0.1272,  0.1665],\n",
       "                      [-0.0256,  0.1182, -0.1136, -0.0341, -0.2101,  0.1110,  0.2271, -0.1558,\n",
       "                       -0.0966, -0.0895,  0.0728,  0.1762,  0.0901, -0.2311,  0.1477, -0.1156,\n",
       "                        0.0025, -0.1139,  0.2052, -0.2300, -0.1313,  0.0358, -0.1171,  0.1062,\n",
       "                        0.0446, -0.0782, -0.0848, -0.3283, -0.1810,  0.0273, -0.0015, -0.0735],\n",
       "                      [ 0.0505,  0.0990,  0.1159, -0.2248, -0.0884,  0.0524,  0.0747, -0.1583,\n",
       "                       -0.0754,  0.1471, -0.1642, -0.1428, -0.1317, -0.1423, -0.2660, -0.1064,\n",
       "                       -0.2475, -0.1962, -0.1146, -0.0475, -0.1570,  0.0640,  0.1290,  0.0815,\n",
       "                        0.0117,  0.1120, -0.2727, -0.2188, -0.1845, -0.1937,  0.0185,  0.0385],\n",
       "                      [-0.0300, -0.1534, -0.0081,  0.0783,  0.1555, -0.0088,  0.1164, -0.1802,\n",
       "                       -0.2278, -0.0679,  0.2545, -0.2686,  0.0174, -0.1589, -0.1903, -0.0579,\n",
       "                       -0.0287,  0.1325, -0.1963, -0.2094,  0.1044, -0.2158, -0.1504, -0.1654,\n",
       "                        0.0640, -0.1120, -0.0188,  0.1335,  0.0828, -0.2448,  0.0313, -0.1918],\n",
       "                      [ 0.1455, -0.1760, -0.2162,  0.1746,  0.0243,  0.1397, -0.1298,  0.2113,\n",
       "                        0.0977,  0.0305, -0.0418,  0.0258, -0.2651,  0.0401,  0.0006, -0.1011,\n",
       "                        0.2000, -0.2340, -0.1867,  0.0478, -0.2542, -0.1294, -0.1302, -0.0070,\n",
       "                       -0.0908,  0.0299,  0.0954, -0.0419, -0.1808, -0.0608, -0.1325,  0.1940],\n",
       "                      [ 0.0709,  0.1606,  0.0341, -0.1436, -0.3326, -0.2054,  0.0305, -0.1097,\n",
       "                        0.2015, -0.2043, -0.1978, -0.2126, -0.0237, -0.1951,  0.0446, -0.1713,\n",
       "                       -0.1364, -0.0420,  0.0557,  0.2318,  0.1245, -0.2267, -0.1019, -0.0434,\n",
       "                       -0.4149,  0.0852,  0.1931,  0.0354,  0.0137,  0.1198,  0.0608, -0.1461],\n",
       "                      [-0.1558, -0.1371,  0.1036, -0.0761,  0.1052,  0.1048, -0.1609,  0.1838,\n",
       "                       -0.1030,  0.2094, -0.2902,  0.0317, -0.0058, -0.1358,  0.2632,  0.0183,\n",
       "                        0.0512,  0.1895, -0.0752,  0.0539,  0.0737, -0.0372, -0.1307, -0.0696,\n",
       "                       -0.0597, -0.2027, -0.1283, -0.0810, -0.2241, -0.1744, -0.0128, -0.1303],\n",
       "                      [ 0.0581, -0.1224, -0.0950, -0.2679, -0.2210, -0.1410,  0.1973,  0.2361,\n",
       "                       -0.0233, -0.2268, -0.0949,  0.1189,  0.1118,  0.1357,  0.1537, -0.1409,\n",
       "                        0.1855, -0.0777,  0.0237,  0.2064,  0.1270, -0.2772, -0.1748,  0.1257,\n",
       "                       -0.0952, -0.2154, -0.1567, -0.0808, -0.1520,  0.1709, -0.0008, -0.2194]])),\n",
       "             ('linear_4.bias',\n",
       "              tensor([-0.0801, -0.0826,  0.1382, -0.0761, -0.2375,  0.0666,  0.0800, -0.0406,\n",
       "                      -0.0660, -0.0322]))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10210792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear_1': Linear(in_features=784, out_features=256, bias=True),\n",
       " 'linear_2': Linear(in_features=256, out_features=128, bias=True),\n",
       " 'linear_3': Linear(in_features=128, out_features=32, bias=True),\n",
       " 'linear_4': Linear(in_features=32, out_features=10, bias=True),\n",
       " 'relu': ReLU()}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b26e6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "for module in model._modules.keys():\n",
    "    if \"linear\" in module:\n",
    "        prune.l1_unstructured(model._modules[module], name=\"weight\", amount=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1594060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model._modules.keys():\n",
    "    if \"linear\" in module:\n",
    "        prune.remove(model._modules[module], name=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "950b227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_1.bias',\n",
       "              tensor([ 2.0831e-01, -2.1906e-02,  4.1102e-02, -6.6021e-02, -3.6502e-02,\n",
       "                       2.8309e-03,  4.5656e-02, -3.7591e-02, -3.4435e-02, -4.4073e-02,\n",
       "                       2.0710e-02, -1.2697e-01,  1.0828e-02,  1.1019e-01,  4.3496e-03,\n",
       "                       1.7435e-01,  1.3645e-01,  4.6424e-02, -3.3416e-02,  3.0224e-01,\n",
       "                      -2.4514e-01,  1.1537e-01,  3.1745e-02,  1.8782e-01,  7.1748e-02,\n",
       "                       6.9810e-02,  3.8171e-02,  9.6300e-02, -2.6342e-02,  5.9144e-02,\n",
       "                      -1.3105e-01,  1.6288e-01, -4.4566e-02, -1.0080e-01,  4.6183e-02,\n",
       "                       4.4561e-02,  6.9640e-02, -1.9696e-02, -5.0873e-02,  9.1190e-02,\n",
       "                       5.0227e-03, -2.1194e-02, -1.8817e-02,  4.2949e-02,  8.3144e-03,\n",
       "                       3.7185e-02,  1.0158e-01, -4.3520e-02,  5.7717e-02,  9.2958e-02,\n",
       "                       1.9421e-02, -4.1240e-02, -3.5855e-02,  4.2309e-02,  8.9072e-02,\n",
       "                       2.5904e-02, -4.8630e-02,  6.9802e-02,  9.5462e-02,  4.7465e-02,\n",
       "                       1.7486e-01, -1.5489e-03,  9.8336e-03, -5.3384e-02,  5.0363e-02,\n",
       "                       6.1769e-02,  9.0447e-02, -5.1240e-02,  2.2755e-02, -6.3353e-02,\n",
       "                       4.0104e-02,  2.4361e-02,  3.2965e-04, -1.7371e-01, -9.3739e-03,\n",
       "                       1.1761e-01,  2.9562e-02, -7.1733e-02,  6.1905e-02,  8.6663e-02,\n",
       "                       1.1283e-01,  9.8451e-02,  7.5809e-02,  6.0932e-02, -3.8973e-02,\n",
       "                      -7.6128e-02,  1.0953e-01, -3.0884e-02,  1.1268e-01,  7.2977e-02,\n",
       "                       3.4586e-02, -5.2746e-02,  9.4172e-02,  1.2562e-01, -8.0788e-02,\n",
       "                      -2.3597e-02, -5.3058e-02,  6.4265e-02,  9.3932e-02, -3.6464e-02,\n",
       "                       8.0575e-03,  1.8028e-01,  1.0813e-02, -2.1912e-02,  1.2326e-01,\n",
       "                       1.7652e-02,  1.6211e-01,  4.2556e-02,  2.7262e-02, -1.8985e-02,\n",
       "                      -1.0937e-02,  1.0194e-02,  7.2140e-02,  1.1142e-01,  1.2357e-01,\n",
       "                       4.6826e-02,  2.0348e-03, -8.0751e-02, -6.3275e-03,  5.9856e-02,\n",
       "                       6.6909e-02,  6.7775e-02,  1.2074e-01, -3.4098e-02,  8.6744e-02,\n",
       "                       1.0831e-02,  3.1226e-05, -1.4002e-02,  7.3829e-02, -1.2834e-01,\n",
       "                       3.7347e-02,  1.4915e-01, -1.7156e-02,  9.1657e-02, -1.8220e-02,\n",
       "                       2.3701e-02,  7.8859e-02,  1.4680e-02, -5.7241e-02,  3.0019e-02,\n",
       "                       1.8048e-02,  1.3989e-02,  5.5645e-02,  3.2028e-02, -2.0663e-02,\n",
       "                       6.8289e-02,  4.1836e-02, -2.0455e-02,  4.4359e-02,  9.9568e-02,\n",
       "                       1.1892e-01,  1.2051e-01, -9.9061e-03,  1.3584e-01, -2.7104e-03,\n",
       "                      -8.3805e-03, -8.0822e-02,  6.5473e-02,  2.4324e-02, -1.0824e-01,\n",
       "                       1.2616e-01, -5.6514e-02,  3.4270e-03,  8.1665e-02,  1.1606e-02,\n",
       "                      -1.6202e-02, -8.5765e-04, -3.0383e-04, -3.6994e-02,  3.7733e-02,\n",
       "                       4.1110e-03, -6.7256e-02,  2.7990e-02,  8.4855e-02,  2.0173e-01,\n",
       "                       1.5445e-02,  2.9792e-02,  5.2371e-02, -6.6559e-02, -2.7690e-02,\n",
       "                      -5.0713e-02,  1.1409e-01,  3.6224e-02,  8.2893e-02,  4.1552e-02,\n",
       "                      -3.5890e-02,  3.1167e-02, -2.5345e-02,  4.7043e-02, -4.2975e-02,\n",
       "                       1.0798e-01, -6.2903e-03,  1.3070e-01, -3.7411e-02, -4.1536e-02,\n",
       "                      -8.2549e-03,  2.1488e-01,  1.6424e-01,  2.9213e-02,  1.9039e-01,\n",
       "                       1.1160e-01,  2.0328e-01, -9.9565e-03,  7.8236e-02, -9.3225e-02,\n",
       "                      -1.8498e-02,  2.1804e-01,  3.0394e-02,  4.9573e-03, -8.3463e-03,\n",
       "                       1.4855e-01,  8.5468e-02, -2.7448e-03,  1.4853e-01,  1.3592e-01,\n",
       "                       6.4631e-03,  5.4941e-02,  8.5173e-02,  8.3824e-02, -9.3079e-02,\n",
       "                       3.7276e-02,  1.4877e-01, -7.5197e-02, -1.7488e-02,  8.2952e-02,\n",
       "                       1.6451e-01,  8.6441e-02,  4.7446e-02, -2.7876e-03,  1.3815e-01,\n",
       "                      -3.7445e-02,  9.3446e-03,  1.6848e-02, -3.3362e-03,  3.0047e-02,\n",
       "                       3.9414e-02, -8.5678e-02,  4.4577e-02,  1.1983e-02,  7.5498e-02,\n",
       "                       1.0694e-01,  2.9359e-02,  7.6730e-02,  8.3276e-02, -1.4017e-02,\n",
       "                      -1.9898e-04, -1.7315e-01,  6.0177e-02,  1.2704e-01, -3.1585e-02,\n",
       "                      -9.3925e-02, -1.3354e-01, -3.2331e-03, -1.6123e-02, -1.3970e-02,\n",
       "                      -5.6853e-02])),\n",
       "             ('linear_1.weight_orig',\n",
       "              tensor([[ 0.0950,  0.0353,  0.0237,  ..., -0.0350,  0.0286,  0.0392],\n",
       "                      [-0.0000, -0.0829,  0.0000,  ...,  0.0378,  0.0912,  0.0309],\n",
       "                      [-0.0643, -0.0754, -0.0452,  ..., -0.0726, -0.1302, -0.0625],\n",
       "                      ...,\n",
       "                      [-0.0724, -0.0226, -0.0903,  ...,  0.0350,  0.0261, -0.0487],\n",
       "                      [ 0.0541, -0.0775, -0.0612,  ..., -0.0666,  0.0562, -0.0386],\n",
       "                      [ 0.0415,  0.0179, -0.0985,  ..., -0.0396, -0.0000, -0.0418]])),\n",
       "             ('linear_1.weight_mask',\n",
       "              tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [0., 1., 0.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 0., 1.]])),\n",
       "             ('linear_2.bias',\n",
       "              tensor([ 0.0054, -0.0275, -0.0301,  0.0583, -0.0395, -0.0043, -0.0765, -0.0275,\n",
       "                      -0.0186, -0.0434, -0.0420,  0.1306, -0.1059,  0.0491, -0.1058,  0.0954,\n",
       "                      -0.1457, -0.0643,  0.0758,  0.1494,  0.1462,  0.0149,  0.0016,  0.0082,\n",
       "                       0.0335, -0.1176, -0.0492,  0.1768,  0.0974, -0.0311,  0.0433,  0.0360,\n",
       "                       0.0132,  0.1071, -0.0579, -0.0247,  0.0937, -0.2493, -0.0613,  0.0814,\n",
       "                       0.0415, -0.0116, -0.0293,  0.0366,  0.0773,  0.1551, -0.0305,  0.0259,\n",
       "                      -0.0196, -0.1198,  0.1782,  0.0997,  0.0879,  0.0687,  0.1099,  0.1255,\n",
       "                       0.0329, -0.1440, -0.0410,  0.0144, -0.0585, -0.2047, -0.0745,  0.0734,\n",
       "                      -0.0300, -0.0731,  0.0837,  0.0961,  0.0534,  0.0466, -0.1527, -0.0411,\n",
       "                      -0.0540,  0.0527,  0.0086, -0.0553, -0.0482, -0.0199,  0.2354,  0.1751,\n",
       "                       0.1276,  0.0688, -0.0535, -0.0204, -0.0135, -0.0568,  0.0887,  0.0503,\n",
       "                       0.0410,  0.0678,  0.0670,  0.0607,  0.1432, -0.0168, -0.0179, -0.0570,\n",
       "                      -0.0427,  0.1472,  0.0021,  0.0652,  0.0780, -0.1356,  0.2597,  0.0282,\n",
       "                      -0.0606,  0.0380,  0.0741,  0.2558,  0.1055,  0.0889, -0.0168, -0.1428,\n",
       "                       0.1903, -0.0442,  0.0441,  0.0403, -0.0481,  0.0432, -0.0382,  0.0104,\n",
       "                       0.0223,  0.1242,  0.1320,  0.0335,  0.0091,  0.0613, -0.0087, -0.1133])),\n",
       "             ('linear_2.weight_orig',\n",
       "              tensor([[-0.0349,  0.0000,  0.0764,  ..., -0.0000, -0.0000, -0.0544],\n",
       "                      [-0.1089, -0.0621,  0.0689,  ...,  0.0355, -0.0285, -0.0421],\n",
       "                      [ 0.0172,  0.0000,  0.0382,  ...,  0.0166,  0.0679, -0.0628],\n",
       "                      ...,\n",
       "                      [ 0.0525,  0.0000,  0.0000,  ...,  0.0238, -0.0303,  0.0851],\n",
       "                      [-0.0476,  0.0748,  0.0532,  ...,  0.0793,  0.0533, -0.0546],\n",
       "                      [-0.0472,  0.0000,  0.0411,  ...,  0.0155,  0.0444,  0.0493]])),\n",
       "             ('linear_2.weight_mask',\n",
       "              tensor([[1., 0., 1.,  ..., 0., 0., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "                      ...,\n",
       "                      [1., 0., 0.,  ..., 1., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [1., 0., 1.,  ..., 1., 1., 1.]])),\n",
       "             ('linear_3.bias',\n",
       "              tensor([ 0.0087, -0.0596, -0.1583,  0.0639,  0.0640, -0.0251,  0.0523, -0.0896,\n",
       "                       0.0754, -0.1632,  0.2041,  0.0034,  0.0155, -0.0851,  0.0390,  0.0183,\n",
       "                      -0.0185,  0.0624,  0.0466, -0.0563,  0.0268,  0.0438,  0.0615, -0.1003,\n",
       "                      -0.1095,  0.1297,  0.2121,  0.1040,  0.0183,  0.0796, -0.0647,  0.1886])),\n",
       "             ('linear_3.weight_orig',\n",
       "              tensor([[-0.0532,  0.0516, -0.0820,  ...,  0.0000,  0.0269, -0.0648],\n",
       "                      [-0.0354,  0.0586, -0.0468,  ..., -0.0460,  0.0000, -0.0588],\n",
       "                      [ 0.0717,  0.1390, -0.0000,  ...,  0.0000, -0.0000,  0.0534],\n",
       "                      ...,\n",
       "                      [ 0.0000, -0.1605, -0.0000,  ..., -0.0000, -0.1324,  0.0000],\n",
       "                      [-0.0000,  0.0735,  0.0553,  ..., -0.0421,  0.0755,  0.0877],\n",
       "                      [-0.0000, -0.0994,  0.0393,  ..., -0.0297, -0.0000, -0.0759]])),\n",
       "             ('linear_3.weight_mask',\n",
       "              tensor([[1., 1., 1.,  ..., 0., 1., 1.],\n",
       "                      [1., 1., 1.,  ..., 1., 0., 1.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 1.],\n",
       "                      ...,\n",
       "                      [0., 1., 0.,  ..., 0., 1., 0.],\n",
       "                      [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "                      [0., 1., 1.,  ..., 1., 0., 1.]])),\n",
       "             ('linear_4.bias',\n",
       "              tensor([-0.0801, -0.0826,  0.1382, -0.0761, -0.2375,  0.0666,  0.0800, -0.0406,\n",
       "                      -0.0660, -0.0322])),\n",
       "             ('linear_4.weight_orig',\n",
       "              tensor([[ 0.0000, -0.0791, -0.1900,  0.0000,  0.1804, -0.0636, -0.1733, -0.0797,\n",
       "                        0.0656,  0.1761,  0.0498,  0.1065, -0.2504,  0.0647, -0.2720, -0.0000,\n",
       "                        0.1117,  0.1532, -0.2115,  0.0000, -0.0985, -0.0572,  0.1266,  0.1868,\n",
       "                        0.0918, -0.1729, -0.2314,  0.0482,  0.2166,  0.0000, -0.0925,  0.0000],\n",
       "                      [-0.0750, -0.0000,  0.0000,  0.1733, -0.0801, -0.1216, -0.0669, -0.1936,\n",
       "                        0.1952,  0.0000, -0.3134, -0.0000,  0.0000, -0.0746,  0.1219, -0.1218,\n",
       "                       -0.0945, -0.2718, -0.0000,  0.2203,  0.0000, -0.0931, -0.0000, -0.1356,\n",
       "                        0.1747,  0.0000,  0.1309, -0.2880,  0.0000, -0.2175, -0.0000, -0.0000],\n",
       "                      [ 0.1536,  0.0874,  0.0000, -0.2124,  0.0868, -0.1359, -0.0872, -0.1166,\n",
       "                        0.0777, -0.1130, -0.1094,  0.0481,  0.0000,  0.1471,  0.0789,  0.1202,\n",
       "                       -0.0000, -0.0640, -0.0000, -0.0951, -0.1126,  0.1818,  0.1655,  0.1322,\n",
       "                       -0.2219, -0.0473, -0.1954,  0.0497,  0.2327, -0.1496,  0.1272,  0.1665],\n",
       "                      [-0.0000,  0.1182, -0.1136, -0.0000, -0.2101,  0.1110,  0.2271, -0.1558,\n",
       "                       -0.0966, -0.0895,  0.0728,  0.1762,  0.0901, -0.2311,  0.1477, -0.1156,\n",
       "                        0.0000, -0.1139,  0.2052, -0.2300, -0.1313,  0.0000, -0.1171,  0.1062,\n",
       "                        0.0000, -0.0782, -0.0848, -0.3283, -0.1810,  0.0000, -0.0000, -0.0735],\n",
       "                      [ 0.0505,  0.0990,  0.1159, -0.2248, -0.0884,  0.0524,  0.0747, -0.1583,\n",
       "                       -0.0754,  0.1471, -0.1642, -0.1428, -0.1317, -0.1423, -0.2660, -0.1064,\n",
       "                       -0.2475, -0.1962, -0.1146, -0.0475, -0.1570,  0.0640,  0.1290,  0.0815,\n",
       "                        0.0000,  0.1120, -0.2727, -0.2188, -0.1845, -0.1937,  0.0000,  0.0000],\n",
       "                      [-0.0000, -0.1534, -0.0000,  0.0783,  0.1555, -0.0000,  0.1164, -0.1802,\n",
       "                       -0.2278, -0.0679,  0.2545, -0.2686,  0.0000, -0.1589, -0.1903, -0.0579,\n",
       "                       -0.0000,  0.1325, -0.1963, -0.2094,  0.1044, -0.2158, -0.1504, -0.1654,\n",
       "                        0.0640, -0.1120, -0.0000,  0.1335,  0.0828, -0.2448,  0.0000, -0.1918],\n",
       "                      [ 0.1455, -0.1760, -0.2162,  0.1746,  0.0000,  0.1397, -0.1298,  0.2113,\n",
       "                        0.0977,  0.0000, -0.0000,  0.0000, -0.2651,  0.0000,  0.0000, -0.1011,\n",
       "                        0.2000, -0.2340, -0.1867,  0.0478, -0.2542, -0.1294, -0.1302, -0.0000,\n",
       "                       -0.0908,  0.0000,  0.0954, -0.0000, -0.1808, -0.0608, -0.1325,  0.1940],\n",
       "                      [ 0.0709,  0.1606,  0.0000, -0.1436, -0.3326, -0.2054,  0.0000, -0.1097,\n",
       "                        0.2015, -0.2043, -0.1978, -0.2126, -0.0000, -0.1951,  0.0000, -0.1713,\n",
       "                       -0.1364, -0.0000,  0.0557,  0.2318,  0.1245, -0.2267, -0.1019, -0.0000,\n",
       "                       -0.4149,  0.0852,  0.1931,  0.0000,  0.0000,  0.1198,  0.0608, -0.1461],\n",
       "                      [-0.1558, -0.1371,  0.1036, -0.0761,  0.1052,  0.1048, -0.1609,  0.1838,\n",
       "                       -0.1030,  0.2094, -0.2902,  0.0000, -0.0000, -0.1358,  0.2632,  0.0000,\n",
       "                        0.0512,  0.1895, -0.0752,  0.0539,  0.0737, -0.0000, -0.1307, -0.0696,\n",
       "                       -0.0597, -0.2027, -0.1283, -0.0810, -0.2241, -0.1744, -0.0000, -0.1303],\n",
       "                      [ 0.0581, -0.1224, -0.0950, -0.2679, -0.2210, -0.1410,  0.1973,  0.2361,\n",
       "                       -0.0000, -0.2268, -0.0949,  0.1189,  0.1118,  0.1357,  0.1537, -0.1409,\n",
       "                        0.1855, -0.0777,  0.0000,  0.2064,  0.1270, -0.2772, -0.1748,  0.1257,\n",
       "                       -0.0952, -0.2154, -0.1567, -0.0808, -0.1520,  0.1709, -0.0000, -0.2194]])),\n",
       "             ('linear_4.weight_mask',\n",
       "              tensor([[0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "                       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.],\n",
       "                      [1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "                       0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.],\n",
       "                      [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "                       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "                      [0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "                       1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1.],\n",
       "                      [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "                      [0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.],\n",
       "                      [1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n",
       "                      [1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "                       1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "                       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "                      [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]]))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cd5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
